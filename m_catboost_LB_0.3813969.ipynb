{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import data\n",
      "merge orders and order_products__prior\n",
      "size of prd features : 5\n",
      "size of users features : 13\n",
      "nb of usersXproducts features : 13\n",
      "merge prod, user and usersXprod feature on usersXprod\n",
      "order row 2000000\n",
      "order row 4000000\n",
      "order row 6000000\n",
      "order row 8000000\n",
      "order row 10000000\n",
      "order row 12000000\n",
      "shape of usersXprod : (13307953, 32)\n"
     ]
    }
   ],
   "source": [
    "# 變數前處理\n",
    "\n",
    "%run preprocessing.py\n",
    "print (\"shape of usersXprod :\", usersXprod.shape)\n",
    "\n",
    "p = pd.read_csv(\"products.csv\")\n",
    "p = p.drop([\"product_name\"], axis=1)\n",
    "usersXprod = pd.merge(usersXprod, p, how='left', on=['product_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and test\n",
    "\n",
    "X_test = usersXprod.loc[usersXprod.eval_set == \"test\", :].copy()\n",
    "X_test.drop(['eval_set'], axis=1, inplace = True)\n",
    "\n",
    "train = usersXprod.loc[usersXprod.eval_set == \"train\", :].copy()\n",
    "train.drop(['eval_set'], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set convert to form of submit\n",
    "\n",
    "train_details = ordert[[\"user_id\", \"order_id\", \"product_id\", \"reordered\"]]\n",
    "\n",
    "try:\n",
    "    df_train_gt = pd.read_csv('train.csv', index_col='order_id')\n",
    "except:\n",
    "    train_gtl = []\n",
    "\n",
    "    for uid, subset in train_details.groupby('user_id'):\n",
    "        subset1 = subset[subset.reordered == 1]\n",
    "        oid = subset.order_id.values[0]\n",
    "\n",
    "        if len(subset1) == 0:\n",
    "            train_gtl.append((oid, 'None'))\n",
    "            continue\n",
    "\n",
    "        ostr = ' '.join([str(int(e)) for e in subset1.product_id.values])\n",
    "        # .strip is needed because join can have a padding space at the end\n",
    "        train_gtl.append((oid, ostr.strip()))\n",
    "\n",
    "    df_train_gt = pd.DataFrame(train_gtl)\n",
    "\n",
    "    df_train_gt.columns = ['order_id', 'products']\n",
    "    df_train_gt.set_index('order_id', inplace=True)\n",
    "    df_train_gt.sort_index(inplace=True)\n",
    "    \n",
    "    df_train_gt.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier as cbc\n",
    "import time\n",
    "\n",
    "def catboost_cv(X_train, y_train, X_val, y_val, features_to_use):\n",
    "    cb = cbc(iterations = 100,\n",
    "             thread_count = 8, \n",
    "             verbose = True,\n",
    "             random_seed = 42,\n",
    "             learning_rate = 0.05,\n",
    "             use_best_model = True,\n",
    "             depth = 8,\n",
    "             fold_permutation_block_size = 64,\n",
    "             calc_feature_importance = True,\n",
    "             leaf_estimation_method = 'Gradient')\n",
    "\n",
    "    cb.fit(X = X_train[features_to_use].values,\n",
    "           y = y_train.values, \n",
    "           cat_features = [X_train[features_to_use].columns.get_loc('aisle_id')], \n",
    "           eval_set = [X_val[features_to_use].values, y_val.values],\n",
    "           verbose = True,\n",
    "           #plot=True\n",
    "          )\n",
    "    return cb\n",
    "\n",
    "# 計時\n",
    "class tick_tock:\n",
    "    def __init__(self, process_name, verbose=1):\n",
    "        self.process_name = process_name\n",
    "        self.verbose = verbose\n",
    "    def __enter__(self):\n",
    "        if self.verbose:\n",
    "            print(self.process_name + \" begin ......\")\n",
    "            self.begin_time = time.time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.verbose:\n",
    "            end_time = time.time()\n",
    "            print(self.process_name + \" end ......\")\n",
    "            print('time lapsing {0} s \\n'.format(end_time - self.begin_time))\n",
    "\n",
    "# 整理成submit形式\n",
    "def ka_add_groupby_features_n_vs_1(df, group_columns_list, target_columns_list, methods_list, keep_only_stats=True, verbose=1):\n",
    "    '''Create statistical columns, group by [N columns] and compute stats on [1 column]\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       df: pandas dataframe\n",
    "          Features matrix\n",
    "       group_columns_list: list_like\n",
    "          List of columns you want to group with, could be multiple columns\n",
    "       target_columns_list: list_like\n",
    "          column you want to compute stats, need to be a list with only one element\n",
    "       methods_list: list_like\n",
    "          methods that you want to use, all methods that supported by groupby in Pandas\n",
    "\n",
    "       Return\n",
    "       ------\n",
    "       new pandas dataframe with original columns and new added columns\n",
    "\n",
    "       Example\n",
    "       -------\n",
    "       ka_add_stats_features_n_vs_1(train, group_columns_list=['x0'], target_columns_list=['x10'])\n",
    "    '''\n",
    "    with tick_tock(\"add stats features\", verbose):\n",
    "        dicts = {\"group_columns_list\": group_columns_list , \"target_columns_list\": target_columns_list, \"methods_list\" :methods_list}\n",
    "\n",
    "        for k, v in dicts.items():\n",
    "            try:\n",
    "                if type(v) == list:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(k + \"should be a list\")\n",
    "            except TypeError as e:\n",
    "                print(e)\n",
    "                raise\n",
    "\n",
    "        grouped_name = ''.join(group_columns_list)\n",
    "        target_name = ''.join(target_columns_list)\n",
    "        combine_name = [[grouped_name] + [method_name] + [target_name] for method_name in methods_list]\n",
    "\n",
    "        df_new = df.copy()\n",
    "        grouped = df_new.groupby(group_columns_list)\n",
    "\n",
    "        the_stats = grouped[target_name].agg(methods_list).reset_index()\n",
    "        the_stats.columns = [grouped_name] + \\\n",
    "                            ['_%s_%s_by_%s' % (grouped_name, method_name, target_name) \\\n",
    "                             for (grouped_name, method_name, target_name) in combine_name]\n",
    "        if keep_only_stats:\n",
    "            return the_stats\n",
    "        else:\n",
    "            df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n",
    "        return df_new\n",
    "\n",
    "# prediction compare with train\n",
    "def compare_results(df_gt, df_preds):\n",
    "    \n",
    "    df_gt_cut = df_gt.loc[df_preds.index]\n",
    "    \n",
    "    f1 = []\n",
    "    for gt, pred in zip(df_gt_cut.sort_index().products, df_preds.sort_index().products):\n",
    "        lgt = gt.replace(\"None\", \"-1\").split(' ')\n",
    "        lpred = pred.replace(\"None\", \"-1\").split(' ')\n",
    "\n",
    "        rr = (np.intersect1d(lgt, lpred))\n",
    "        precision = np.float(len(rr)) / len(lpred)\n",
    "        recall = np.float(len(rr)) / len(lgt)\n",
    "\n",
    "        denom = precision + recall\n",
    "        f1.append(((2 * precision * recall) / denom) if denom > 0 else 0)\n",
    "    \n",
    "    return(np.mean(f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borders generated\n",
      "0:\tlearn 0.6350273768\ttest 0.6349723465\tbestTest 0.6349723465\t\ttotal: 53.4s\tremaining: 1h 28m 9s\n",
      "1:\tlearn 0.5846795989\ttest 0.5845703567\tbestTest 0.5845703567\t\ttotal: 1m 17s\tremaining: 1h 3m 28s\n",
      "2:\tlearn 0.5419232682\ttest 0.5417262977\tbestTest 0.5417262977\t\ttotal: 1m 43s\tremaining: 55m 42s\n",
      "3:\tlearn 0.5046411156\ttest 0.5043733238\tbestTest 0.5043733238\t\ttotal: 2m 7s\tremaining: 51m 4s\n",
      "4:\tlearn 0.4726605466\ttest 0.4723389751\tbestTest 0.4723389751\t\ttotal: 2m 33s\tremaining: 48m 29s\n",
      "5:\tlearn 0.4446332698\ttest 0.4442886982\tbestTest 0.4442886982\t\ttotal: 3m\tremaining: 47m 7s\n",
      "6:\tlearn 0.4203329539\ttest 0.4199755293\tbestTest 0.4199755293\t\ttotal: 3m 25s\tremaining: 45m 24s\n",
      "7:\tlearn 0.399087304\ttest 0.3987097604\tbestTest 0.3987097604\t\ttotal: 6m 42s\tremaining: 1h 17m 12s\n",
      "8:\tlearn 0.3807681134\ttest 0.3803732151\tbestTest 0.3803732151\t\ttotal: 7m 10s\tremaining: 1h 12m 34s\n",
      "9:\tlearn 0.3649281875\ttest 0.3645250806\tbestTest 0.3645250806\t\ttotal: 7m 34s\tremaining: 1h 8m 14s\n",
      "10:\tlearn 0.3514975282\ttest 0.3510796984\tbestTest 0.3510796984\t\ttotal: 8m 1s\tremaining: 1h 4m 51s\n",
      "11:\tlearn 0.3396397405\ttest 0.3392184715\tbestTest 0.3392184715\t\ttotal: 8m 24s\tremaining: 1h 1m 42s\n",
      "12:\tlearn 0.329453781\ttest 0.3290292379\tbestTest 0.3290292379\t\ttotal: 8m 49s\tremaining: 59m 2s\n",
      "13:\tlearn 0.3203636238\ttest 0.3199270362\tbestTest 0.3199270362\t\ttotal: 9m 12s\tremaining: 56m 33s\n",
      "14:\tlearn 0.3124321832\ttest 0.311992637\tbestTest 0.311992637\t\ttotal: 9m 38s\tremaining: 54m 36s\n",
      "15:\tlearn 0.3054600245\ttest 0.3050164712\tbestTest 0.3050164712\t\ttotal: 10m 2s\tremaining: 52m 41s\n",
      "16:\tlearn 0.2993526806\ttest 0.29891432\tbestTest 0.29891432\t\ttotal: 10m 27s\tremaining: 51m 2s\n",
      "17:\tlearn 0.2940373286\ttest 0.2935884216\tbestTest 0.2935884216\t\ttotal: 10m 52s\tremaining: 49m 33s\n",
      "18:\tlearn 0.2893466913\ttest 0.2888859931\tbestTest 0.2888859931\t\ttotal: 11m 17s\tremaining: 48m 7s\n",
      "19:\tlearn 0.2852638864\ttest 0.2847949613\tbestTest 0.2847949613\t\ttotal: 11m 41s\tremaining: 46m 47s\n",
      "20:\tlearn 0.2815256165\ttest 0.2810477292\tbestTest 0.2810477292\t\ttotal: 12m 6s\tremaining: 45m 32s\n",
      "21:\tlearn 0.2782485166\ttest 0.277758395\tbestTest 0.277758395\t\ttotal: 12m 30s\tremaining: 44m 21s\n",
      "22:\tlearn 0.2753364536\ttest 0.2748393239\tbestTest 0.2748393239\t\ttotal: 12m 54s\tremaining: 43m 13s\n",
      "23:\tlearn 0.2727601473\ttest 0.2722543231\tbestTest 0.2722543231\t\ttotal: 13m 17s\tremaining: 42m 6s\n",
      "24:\tlearn 0.270462863\ttest 0.2699537714\tbestTest 0.2699537714\t\ttotal: 13m 41s\tremaining: 41m 4s\n",
      "25:\tlearn 0.2684851682\ttest 0.2679783974\tbestTest 0.2679783974\t\ttotal: 14m 4s\tremaining: 40m 3s\n",
      "26:\tlearn 0.2667560595\ttest 0.2662457027\tbestTest 0.2662457027\t\ttotal: 14m 28s\tremaining: 39m 8s\n",
      "27:\tlearn 0.2650058296\ttest 0.2644896385\tbestTest 0.2644896385\t\ttotal: 14m 52s\tremaining: 38m 16s\n",
      "28:\tlearn 0.2635410668\ttest 0.2630152839\tbestTest 0.2630152839\t\ttotal: 15m 16s\tremaining: 37m 24s\n",
      "29:\tlearn 0.2622536545\ttest 0.2617259084\tbestTest 0.2617259084\t\ttotal: 15m 40s\tremaining: 36m 34s\n",
      "30:\tlearn 0.2610418677\ttest 0.2605092923\tbestTest 0.2605092923\t\ttotal: 16m 4s\tremaining: 35m 45s\n",
      "31:\tlearn 0.2600046733\ttest 0.2594738508\tbestTest 0.2594738508\t\ttotal: 16m 27s\tremaining: 34m 59s\n",
      "32:\tlearn 0.2590845114\ttest 0.2585484635\tbestTest 0.2585484635\t\ttotal: 16m 51s\tremaining: 34m 13s\n",
      "33:\tlearn 0.2581820875\ttest 0.2576425466\tbestTest 0.2576425466\t\ttotal: 17m 16s\tremaining: 33m 31s\n",
      "34:\tlearn 0.2574218182\ttest 0.2568830694\tbestTest 0.2568830694\t\ttotal: 17m 41s\tremaining: 32m 50s\n",
      "35:\tlearn 0.2566140312\ttest 0.2560630709\tbestTest 0.2560630709\t\ttotal: 18m 4s\tremaining: 32m 7s\n",
      "36:\tlearn 0.2561155719\ttest 0.255568362\tbestTest 0.255568362\t\ttotal: 18m 26s\tremaining: 31m 24s\n",
      "37:\tlearn 0.2555882444\ttest 0.2550438933\tbestTest 0.2550438933\t\ttotal: 18m 51s\tremaining: 30m 45s\n",
      "38:\tlearn 0.254946813\ttest 0.2543971147\tbestTest 0.2543971147\t\ttotal: 19m 16s\tremaining: 30m 8s\n",
      "39:\tlearn 0.2543303817\ttest 0.2537767167\tbestTest 0.2537767167\t\ttotal: 19m 42s\tremaining: 29m 33s\n",
      "40:\tlearn 0.2538652904\ttest 0.2533026338\tbestTest 0.2533026338\t\ttotal: 20m 9s\tremaining: 29m\n",
      "41:\tlearn 0.253379844\ttest 0.2528119311\tbestTest 0.2528119311\t\ttotal: 20m 35s\tremaining: 28m 25s\n",
      "42:\tlearn 0.2529718855\ttest 0.2524038604\tbestTest 0.2524038604\t\ttotal: 20m 58s\tremaining: 27m 48s\n",
      "43:\tlearn 0.2526195708\ttest 0.2520528161\tbestTest 0.2520528161\t\ttotal: 21m 20s\tremaining: 27m 10s\n",
      "44:\tlearn 0.2523038953\ttest 0.2517340622\tbestTest 0.2517340622\t\ttotal: 21m 44s\tremaining: 26m 34s\n",
      "45:\tlearn 0.2519429549\ttest 0.2513692015\tbestTest 0.2513692015\t\ttotal: 22m 7s\tremaining: 25m 58s\n",
      "46:\tlearn 0.2516981202\ttest 0.2511234404\tbestTest 0.2511234404\t\ttotal: 22m 31s\tremaining: 25m 24s\n",
      "47:\tlearn 0.2513912444\ttest 0.2508149481\tbestTest 0.2508149481\t\ttotal: 22m 55s\tremaining: 24m 49s\n",
      "48:\tlearn 0.2511004501\ttest 0.2505227639\tbestTest 0.2505227639\t\ttotal: 23m 18s\tremaining: 24m 15s\n",
      "49:\tlearn 0.2508261819\ttest 0.2502436723\tbestTest 0.2502436723\t\ttotal: 23m 42s\tremaining: 23m 42s\n",
      "50:\tlearn 0.2506044075\ttest 0.2500217945\tbestTest 0.2500217945\t\ttotal: 24m 5s\tremaining: 23m 9s\n",
      "51:\tlearn 0.2503938027\ttest 0.2498133977\tbestTest 0.2498133977\t\ttotal: 24m 28s\tremaining: 22m 35s\n",
      "52:\tlearn 0.2502265488\ttest 0.2496433059\tbestTest 0.2496433059\t\ttotal: 24m 51s\tremaining: 22m 2s\n",
      "53:\tlearn 0.2500938995\ttest 0.2495114115\tbestTest 0.2495114115\t\ttotal: 25m 14s\tremaining: 21m 29s\n",
      "54:\tlearn 0.2499310237\ttest 0.2493487911\tbestTest 0.2493487911\t\ttotal: 25m 37s\tremaining: 20m 58s\n",
      "55:\tlearn 0.2497550416\ttest 0.2491714132\tbestTest 0.2491714132\t\ttotal: 26m\tremaining: 20m 26s\n",
      "56:\tlearn 0.249604694\ttest 0.2490205939\tbestTest 0.2490205939\t\ttotal: 26m 24s\tremaining: 19m 55s\n",
      "57:\tlearn 0.2494350429\ttest 0.2488470903\tbestTest 0.2488470903\t\ttotal: 26m 48s\tremaining: 19m 24s\n",
      "58:\tlearn 0.2492970301\ttest 0.2487097728\tbestTest 0.2487097728\t\ttotal: 27m 11s\tremaining: 18m 53s\n",
      "59:\tlearn 0.2491856401\ttest 0.2485995367\tbestTest 0.2485995367\t\ttotal: 27m 36s\tremaining: 18m 24s\n",
      "60:\tlearn 0.2490536199\ttest 0.2484700202\tbestTest 0.2484700202\t\ttotal: 28m\tremaining: 17m 54s\n",
      "61:\tlearn 0.2489376679\ttest 0.2483552954\tbestTest 0.2483552954\t\ttotal: 28m 24s\tremaining: 17m 24s\n",
      "62:\tlearn 0.248844503\ttest 0.248260331\tbestTest 0.248260331\t\ttotal: 28m 50s\tremaining: 16m 56s\n",
      "63:\tlearn 0.2487383216\ttest 0.2481541172\tbestTest 0.2481541172\t\ttotal: 29m 16s\tremaining: 16m 27s\n",
      "64:\tlearn 0.2486488165\ttest 0.248065598\tbestTest 0.248065598\t\ttotal: 29m 40s\tremaining: 15m 58s\n",
      "65:\tlearn 0.2485515804\ttest 0.2479659683\tbestTest 0.2479659683\t\ttotal: 30m 3s\tremaining: 15m 29s\n",
      "66:\tlearn 0.2484582653\ttest 0.2478705644\tbestTest 0.2478705644\t\ttotal: 30m 28s\tremaining: 15m\n",
      "67:\tlearn 0.2483892585\ttest 0.2478027482\tbestTest 0.2478027482\t\ttotal: 34m 33s\tremaining: 16m 15s\n",
      "68:\tlearn 0.2483196942\ttest 0.2477333349\tbestTest 0.2477333349\t\ttotal: 34m 56s\tremaining: 15m 41s\n",
      "69:\tlearn 0.2482537649\ttest 0.2476679511\tbestTest 0.2476679511\t\ttotal: 35m 20s\tremaining: 15m 8s\n",
      "70:\tlearn 0.2481987589\ttest 0.2476128891\tbestTest 0.2476128891\t\ttotal: 35m 44s\tremaining: 14m 35s\n",
      "71:\tlearn 0.2481280098\ttest 0.2475427772\tbestTest 0.2475427772\t\ttotal: 36m 10s\tremaining: 14m 4s\n",
      "72:\tlearn 0.2480565784\ttest 0.2474731157\tbestTest 0.2474731157\t\ttotal: 36m 35s\tremaining: 13m 31s\n",
      "73:\tlearn 0.2479988139\ttest 0.2474153029\tbestTest 0.2474153029\t\ttotal: 36m 59s\tremaining: 12m 59s\n",
      "74:\tlearn 0.2479414625\ttest 0.2473598185\tbestTest 0.2473598185\t\ttotal: 37m 24s\tremaining: 12m 28s\n",
      "75:\tlearn 0.2478792837\ttest 0.2472964497\tbestTest 0.2472964497\t\ttotal: 37m 49s\tremaining: 11m 56s\n",
      "76:\tlearn 0.2478259425\ttest 0.2472447653\tbestTest 0.2472447653\t\ttotal: 38m 13s\tremaining: 11m 25s\n",
      "77:\tlearn 0.247768403\ttest 0.2471865903\tbestTest 0.2471865903\t\ttotal: 38m 38s\tremaining: 10m 53s\n",
      "78:\tlearn 0.2477238483\ttest 0.2471426186\tbestTest 0.2471426186\t\ttotal: 39m 2s\tremaining: 10m 22s\n",
      "79:\tlearn 0.2476801338\ttest 0.2470996954\tbestTest 0.2470996954\t\ttotal: 39m 25s\tremaining: 9m 51s\n",
      "80:\tlearn 0.2476325671\ttest 0.2470494711\tbestTest 0.2470494711\t\ttotal: 39m 50s\tremaining: 9m 20s\n",
      "81:\tlearn 0.2475893525\ttest 0.2470091258\tbestTest 0.2470091258\t\ttotal: 40m 13s\tremaining: 8m 49s\n",
      "82:\tlearn 0.2475412062\ttest 0.2469599672\tbestTest 0.2469599672\t\ttotal: 40m 38s\tremaining: 8m 19s\n",
      "83:\tlearn 0.2475108674\ttest 0.2469299629\tbestTest 0.2469299629\t\ttotal: 41m 1s\tremaining: 7m 48s\n",
      "84:\tlearn 0.2474733018\ttest 0.2468948639\tbestTest 0.2468948639\t\ttotal: 41m 25s\tremaining: 7m 18s\n",
      "85:\tlearn 0.2474321928\ttest 0.2468620557\tbestTest 0.2468620557\t\ttotal: 41m 48s\tremaining: 6m 48s\n",
      "86:\tlearn 0.2473877287\ttest 0.2468189226\tbestTest 0.2468189226\t\ttotal: 42m 13s\tremaining: 6m 18s\n",
      "87:\tlearn 0.2473497518\ttest 0.2467810744\tbestTest 0.2467810744\t\ttotal: 42m 37s\tremaining: 5m 48s\n",
      "88:\tlearn 0.2473102343\ttest 0.2467413232\tbestTest 0.2467413232\t\ttotal: 43m 1s\tremaining: 5m 19s\n",
      "89:\tlearn 0.2472753264\ttest 0.2467067406\tbestTest 0.2467067406\t\ttotal: 43m 26s\tremaining: 4m 49s\n",
      "90:\tlearn 0.2472392715\ttest 0.2466699536\tbestTest 0.2466699536\t\ttotal: 43m 49s\tremaining: 4m 20s\n",
      "91:\tlearn 0.2472043217\ttest 0.2466368745\tbestTest 0.2466368745\t\ttotal: 44m 13s\tremaining: 3m 50s\n",
      "92:\tlearn 0.2471688677\ttest 0.2466029501\tbestTest 0.2466029501\t\ttotal: 44m 38s\tremaining: 3m 21s\n",
      "93:\tlearn 0.2471420787\ttest 0.2465787183\tbestTest 0.2465787183\t\ttotal: 1h 4m 41s\tremaining: 4m 7s\n",
      "94:\tlearn 0.2471167945\ttest 0.2465573504\tbestTest 0.2465573504\t\ttotal: 1h 5m 14s\tremaining: 3m 26s\n",
      "95:\tlearn 0.2470802231\ttest 0.2465212816\tbestTest 0.2465212816\t\ttotal: 1h 5m 43s\tremaining: 2m 44s\n",
      "96:\tlearn 0.2470467216\ttest 0.2464892306\tbestTest 0.2464892306\t\ttotal: 1h 6m 7s\tremaining: 2m 2s\n",
      "97:\tlearn 0.2470118401\ttest 0.2464560926\tbestTest 0.2464560926\t\ttotal: 1h 6m 31s\tremaining: 1m 21s\n",
      "98:\tlearn 0.2469810185\ttest 0.2464274111\tbestTest 0.2464274111\t\ttotal: 1h 6m 56s\tremaining: 40.6s\n",
      "99:\tlearn 0.2469544906\ttest 0.2464010712\tbestTest 0.2464010712\t\ttotal: 1h 7m 21s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2464010712\n",
      "bestIteration = 99\n",
      "\n",
      "Shrink model to first 100 iterations.\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 2.012153148651123 s \n",
      "\n",
      "0 0.378282030441\n",
      "Borders generated\n",
      "0:\tlearn 0.6350149769\ttest 0.6351953977\tbestTest 0.6351953977\t\ttotal: 55.6s\tremaining: 1h 31m 41s\n",
      "1:\tlearn 0.5848190369\ttest 0.585115033\tbestTest 0.585115033\t\ttotal: 1m 22s\tremaining: 1h 7m 25s\n",
      "2:\tlearn 0.5412943806\ttest 0.5416958351\tbestTest 0.5416958351\t\ttotal: 1m 49s\tremaining: 58m 55s\n",
      "3:\tlearn 0.5039499838\ttest 0.5044546875\tbestTest 0.5044546875\t\ttotal: 2m 17s\tremaining: 55m 2s\n",
      "4:\tlearn 0.4711864948\ttest 0.4717599296\tbestTest 0.4717599296\t\ttotal: 2m 41s\tremaining: 51m 10s\n",
      "5:\tlearn 0.4436646846\ttest 0.444311526\tbestTest 0.444311526\t\ttotal: 3m 5s\tremaining: 48m 23s\n",
      "6:\tlearn 0.4197173155\ttest 0.4204094886\tbestTest 0.4204094886\t\ttotal: 3m 29s\tremaining: 46m 29s\n",
      "7:\tlearn 0.3992892784\ttest 0.40002917\tbestTest 0.40002917\t\ttotal: 3m 54s\tremaining: 44m 58s\n",
      "8:\tlearn 0.381013441\ttest 0.3817902077\tbestTest 0.3817902077\t\ttotal: 4m 17s\tremaining: 43m 23s\n",
      "9:\tlearn 0.3649904626\ttest 0.3657999621\tbestTest 0.3657999621\t\ttotal: 4m 40s\tremaining: 42m 8s\n",
      "10:\tlearn 0.3513579118\ttest 0.3521956468\tbestTest 0.3521956468\t\ttotal: 5m 5s\tremaining: 41m 8s\n",
      "11:\tlearn 0.3396718609\ttest 0.3405299089\tbestTest 0.3405299089\t\ttotal: 5m 29s\tremaining: 40m 16s\n",
      "12:\tlearn 0.3292999988\ttest 0.3301788721\tbestTest 0.3301788721\t\ttotal: 2h 34m 7s\tremaining: 17h 11m 28s\n",
      "13:\tlearn 0.3203449124\ttest 0.3212405737\tbestTest 0.3212405737\t\ttotal: 2h 34m 34s\tremaining: 15h 49m 28s\n",
      "14:\tlearn 0.3125567979\ttest 0.3134639503\tbestTest 0.3134639503\t\ttotal: 2h 34m 59s\tremaining: 14h 38m 16s\n",
      "15:\tlearn 0.305742896\ttest 0.3066607052\tbestTest 0.3066607052\t\ttotal: 2h 35m 30s\tremaining: 13h 36m 24s\n",
      "16:\tlearn 0.2996284155\ttest 0.3005575106\tbestTest 0.3005575106\t\ttotal: 3h 35m 36s\tremaining: 17h 32m 40s\n",
      "17:\tlearn 0.2941493357\ttest 0.2950895379\tbestTest 0.2950895379\t\ttotal: 3h 36m 2s\tremaining: 16h 24m 9s\n",
      "18:\tlearn 0.2894213615\ttest 0.2903719269\tbestTest 0.2903719269\t\ttotal: 4h 31m 19s\tremaining: 19h 16m 40s\n",
      "19:\tlearn 0.285279762\ttest 0.2862338526\tbestTest 0.2862338526\t\ttotal: 4h 31m 49s\tremaining: 18h 7m 18s\n",
      "20:\tlearn 0.2815571023\ttest 0.2825191311\tbestTest 0.2825191311\t\ttotal: 4h 32m 14s\tremaining: 17h 4m 8s\n",
      "21:\tlearn 0.2781723868\ttest 0.2791387057\tbestTest 0.2791387057\t\ttotal: 4h 32m 42s\tremaining: 16h 6m 53s\n",
      "22:\tlearn 0.2752034709\ttest 0.2761706288\tbestTest 0.2761706288\t\ttotal: 4h 33m 8s\tremaining: 15h 14m 25s\n",
      "23:\tlearn 0.2725615658\ttest 0.2735355714\tbestTest 0.2735355714\t\ttotal: 4h 33m 32s\tremaining: 14h 26m 13s\n",
      "24:\tlearn 0.2703288297\ttest 0.2713077364\tbestTest 0.2713077364\t\ttotal: 4h 33m 56s\tremaining: 13h 41m 49s\n",
      "25:\tlearn 0.268219071\ttest 0.2692036363\tbestTest 0.2692036363\t\ttotal: 4h 34m 19s\tremaining: 13h 45s\n",
      "26:\tlearn 0.266399755\ttest 0.2673821315\tbestTest 0.2673821315\t\ttotal: 4h 34m 43s\tremaining: 12h 22m 46s\n",
      "27:\tlearn 0.2646563824\ttest 0.2656413098\tbestTest 0.2656413098\t\ttotal: 4h 35m 7s\tremaining: 11h 47m 27s\n",
      "28:\tlearn 0.2632216042\ttest 0.264210306\tbestTest 0.264210306\t\ttotal: 4h 35m 31s\tremaining: 11h 14m 33s\n",
      "29:\tlearn 0.2619381759\ttest 0.2629270154\tbestTest 0.2629270154\t\ttotal: 4h 35m 56s\tremaining: 10h 43m 51s\n",
      "30:\tlearn 0.260774678\ttest 0.2617671199\tbestTest 0.2617671199\t\ttotal: 4h 36m 19s\tremaining: 10h 15m 3s\n",
      "31:\tlearn 0.2597137618\ttest 0.2607062717\tbestTest 0.2607062717\t\ttotal: 4h 36m 43s\tremaining: 9h 48m 3s\n",
      "32:\tlearn 0.258755593\ttest 0.2597478163\tbestTest 0.2597478163\t\ttotal: 4h 37m 8s\tremaining: 9h 22m 40s\n",
      "33:\tlearn 0.2577548527\ttest 0.2587475355\tbestTest 0.2587475355\t\ttotal: 4h 37m 32s\tremaining: 8h 58m 44s\n",
      "34:\tlearn 0.256968266\ttest 0.2579660452\tbestTest 0.2579660452\t\ttotal: 4h 37m 55s\tremaining: 8h 36m 9s\n",
      "35:\tlearn 0.2561131323\ttest 0.2571115241\tbestTest 0.2571115241\t\ttotal: 4h 38m 19s\tremaining: 8h 14m 48s\n",
      "36:\tlearn 0.255464216\ttest 0.2564651314\tbestTest 0.2564651314\t\ttotal: 4h 38m 43s\tremaining: 7h 54m 34s\n",
      "37:\tlearn 0.2548538044\ttest 0.2558535925\tbestTest 0.2558535925\t\ttotal: 4h 39m 6s\tremaining: 7h 35m 23s\n",
      "38:\tlearn 0.2542327603\ttest 0.2552345916\tbestTest 0.2552345916\t\ttotal: 4h 39m 31s\tremaining: 7h 17m 12s\n",
      "39:\tlearn 0.2536610923\ttest 0.254664605\tbestTest 0.254664605\t\ttotal: 4h 39m 58s\tremaining: 6h 59m 57s\n",
      "40:\tlearn 0.2531422061\ttest 0.2541494073\tbestTest 0.2541494073\t\ttotal: 4h 40m 24s\tremaining: 6h 43m 31s\n",
      "41:\tlearn 0.2527037263\ttest 0.2537111977\tbestTest 0.2537111977\t\ttotal: 4h 40m 51s\tremaining: 6h 27m 51s\n",
      "42:\tlearn 0.2522686799\ttest 0.2532793764\tbestTest 0.2532793764\t\ttotal: 4h 41m 16s\tremaining: 6h 12m 51s\n",
      "43:\tlearn 0.2520062725\ttest 0.253017227\tbestTest 0.253017227\t\ttotal: 4h 41m 39s\tremaining: 5h 58m 28s\n",
      "44:\tlearn 0.2516529541\ttest 0.2526672014\tbestTest 0.2526672014\t\ttotal: 4h 42m 3s\tremaining: 5h 44m 44s\n",
      "45:\tlearn 0.25134008\ttest 0.2523554653\tbestTest 0.2523554653\t\ttotal: 4h 42m 28s\tremaining: 5h 31m 36s\n",
      "46:\tlearn 0.2510615688\ttest 0.2520749195\tbestTest 0.2520749195\t\ttotal: 4h 42m 50s\tremaining: 5h 18m 57s\n",
      "47:\tlearn 0.2507704198\ttest 0.2517854649\tbestTest 0.2517854649\t\ttotal: 4h 43m 13s\tremaining: 5h 6m 49s\n",
      "48:\tlearn 0.2504783787\ttest 0.2514963823\tbestTest 0.2514963823\t\ttotal: 4h 43m 37s\tremaining: 4h 55m 11s\n",
      "49:\tlearn 0.2502238454\ttest 0.2512431342\tbestTest 0.2512431342\t\ttotal: 4h 44m 7s\tremaining: 4h 44m 7s\n",
      "50:\tlearn 0.2500184153\ttest 0.2510380658\tbestTest 0.2510380658\t\ttotal: 4h 44m 32s\tremaining: 4h 33m 22s\n",
      "51:\tlearn 0.2497907721\ttest 0.25081021\tbestTest 0.25081021\t\ttotal: 4h 44m 56s\tremaining: 4h 23m 1s\n",
      "52:\tlearn 0.2495685463\ttest 0.2505897042\tbestTest 0.2505897042\t\ttotal: 4h 45m 21s\tremaining: 4h 13m 3s\n",
      "53:\tlearn 0.2494010062\ttest 0.2504224336\tbestTest 0.2504224336\t\ttotal: 4h 45m 45s\tremaining: 4h 3m 25s\n",
      "54:\tlearn 0.2492100166\ttest 0.2502322155\tbestTest 0.2502322155\t\ttotal: 4h 46m 8s\tremaining: 3h 54m 6s\n",
      "55:\tlearn 0.2490392871\ttest 0.2500604334\tbestTest 0.2500604334\t\ttotal: 4h 46m 31s\tremaining: 3h 45m 7s\n",
      "56:\tlearn 0.2488755209\ttest 0.2499008408\tbestTest 0.2499008408\t\ttotal: 4h 46m 55s\tremaining: 3h 36m 26s\n",
      "57:\tlearn 0.2487285403\ttest 0.2497581447\tbestTest 0.2497581447\t\ttotal: 4h 47m 18s\tremaining: 3h 28m 3s\n",
      "58:\tlearn 0.2486233779\ttest 0.2496515198\tbestTest 0.2496515198\t\ttotal: 4h 47m 44s\tremaining: 3h 19m 57s\n",
      "59:\tlearn 0.248487709\ttest 0.2495180173\tbestTest 0.2495180173\t\ttotal: 4h 48m 8s\tremaining: 3h 12m 5s\n",
      "60:\tlearn 0.2484006566\ttest 0.2494319787\tbestTest 0.2494319787\t\ttotal: 4h 48m 33s\tremaining: 3h 4m 29s\n",
      "61:\tlearn 0.2482888733\ttest 0.249320329\tbestTest 0.249320329\t\ttotal: 4h 48m 56s\tremaining: 2h 57m 5s\n",
      "62:\tlearn 0.2481874344\ttest 0.2492181758\tbestTest 0.2492181758\t\ttotal: 4h 49m 21s\tremaining: 2h 49m 56s\n",
      "63:\tlearn 0.2480849782\ttest 0.2491157969\tbestTest 0.2491157969\t\ttotal: 4h 49m 47s\tremaining: 2h 43m\n",
      "64:\tlearn 0.2479880137\ttest 0.2490189487\tbestTest 0.2490189487\t\ttotal: 4h 50m 14s\tremaining: 2h 36m 16s\n",
      "65:\tlearn 0.2478970223\ttest 0.2489309494\tbestTest 0.2489309494\t\ttotal: 4h 50m 38s\tremaining: 2h 29m 43s\n",
      "66:\tlearn 0.2478061464\ttest 0.2488411257\tbestTest 0.2488411257\t\ttotal: 4h 51m 2s\tremaining: 2h 23m 20s\n",
      "67:\tlearn 0.2477184395\ttest 0.2487555373\tbestTest 0.2487555373\t\ttotal: 4h 51m 25s\tremaining: 2h 17m 8s\n",
      "68:\tlearn 0.2476404983\ttest 0.248677343\tbestTest 0.248677343\t\ttotal: 4h 51m 49s\tremaining: 2h 11m 6s\n",
      "69:\tlearn 0.2475601708\ttest 0.2485978264\tbestTest 0.2485978264\t\ttotal: 4h 52m 13s\tremaining: 2h 5m 14s\n",
      "70:\tlearn 0.2474826548\ttest 0.2485247651\tbestTest 0.2485247651\t\ttotal: 4h 52m 36s\tremaining: 1h 59m 30s\n",
      "71:\tlearn 0.2474063573\ttest 0.2484520841\tbestTest 0.2484520841\t\ttotal: 4h 52m 59s\tremaining: 1h 53m 56s\n",
      "72:\tlearn 0.2473493078\ttest 0.2483970318\tbestTest 0.2483970318\t\ttotal: 4h 53m 22s\tremaining: 1h 48m 30s\n",
      "73:\tlearn 0.2472980425\ttest 0.2483451452\tbestTest 0.2483451452\t\ttotal: 4h 53m 47s\tremaining: 1h 43m 13s\n",
      "74:\tlearn 0.2472407134\ttest 0.2482885341\tbestTest 0.2482885341\t\ttotal: 4h 54m 11s\tremaining: 1h 38m 3s\n",
      "75:\tlearn 0.2471826992\ttest 0.2482310217\tbestTest 0.2482310217\t\ttotal: 4h 54m 35s\tremaining: 1h 33m 1s\n",
      "76:\tlearn 0.2471405399\ttest 0.2481885789\tbestTest 0.2481885789\t\ttotal: 4h 54m 59s\tremaining: 1h 28m 6s\n",
      "77:\tlearn 0.2470812155\ttest 0.2481300369\tbestTest 0.2481300369\t\ttotal: 4h 55m 23s\tremaining: 1h 23m 19s\n",
      "78:\tlearn 0.2470293669\ttest 0.2480795458\tbestTest 0.2480795458\t\ttotal: 4h 55m 47s\tremaining: 1h 18m 37s\n",
      "79:\tlearn 0.246983607\ttest 0.248036013\tbestTest 0.248036013\t\ttotal: 4h 56m 11s\tremaining: 1h 14m 2s\n",
      "80:\tlearn 0.2469467532\ttest 0.2480052387\tbestTest 0.2480052387\t\ttotal: 4h 56m 34s\tremaining: 1h 9m 34s\n",
      "81:\tlearn 0.2469063153\ttest 0.2479652056\tbestTest 0.2479652056\t\ttotal: 4h 56m 58s\tremaining: 1h 5m 11s\n",
      "82:\tlearn 0.2468692686\ttest 0.2479310084\tbestTest 0.2479310084\t\ttotal: 4h 57m 22s\tremaining: 1h 54s\n",
      "83:\tlearn 0.2468331623\ttest 0.2478971\tbestTest 0.2478971\t\ttotal: 4h 57m 46s\tremaining: 56m 43s\n",
      "84:\tlearn 0.2468002833\ttest 0.2478726932\tbestTest 0.2478726932\t\ttotal: 4h 58m 11s\tremaining: 52m 37s\n",
      "85:\tlearn 0.2467751847\ttest 0.2478500128\tbestTest 0.2478500128\t\ttotal: 4h 58m 38s\tremaining: 48m 37s\n",
      "86:\tlearn 0.2467380089\ttest 0.247813234\tbestTest 0.247813234\t\ttotal: 4h 59m 4s\tremaining: 44m 41s\n",
      "87:\tlearn 0.2466922829\ttest 0.2477703387\tbestTest 0.2477703387\t\ttotal: 4h 59m 28s\tremaining: 40m 50s\n",
      "88:\tlearn 0.2466595443\ttest 0.2477445846\tbestTest 0.2477445846\t\ttotal: 4h 59m 52s\tremaining: 37m 3s\n",
      "89:\tlearn 0.2466283151\ttest 0.2477139809\tbestTest 0.2477139809\t\ttotal: 5h 15s\tremaining: 33m 21s\n",
      "90:\tlearn 0.246588489\ttest 0.2476779697\tbestTest 0.2476779697\t\ttotal: 5h 38s\tremaining: 29m 44s\n",
      "91:\tlearn 0.2465507142\ttest 0.2476411555\tbestTest 0.2476411555\t\ttotal: 5h 1m 2s\tremaining: 26m 10s\n",
      "92:\tlearn 0.2465186723\ttest 0.2476107381\tbestTest 0.2476107381\t\ttotal: 5h 1m 27s\tremaining: 22m 41s\n",
      "93:\tlearn 0.2464827359\ttest 0.2475749646\tbestTest 0.2475749646\t\ttotal: 5h 1m 52s\tremaining: 19m 16s\n",
      "94:\tlearn 0.2464458839\ttest 0.2475381943\tbestTest 0.2475381943\t\ttotal: 5h 2m 17s\tremaining: 15m 54s\n",
      "95:\tlearn 0.2464140402\ttest 0.2475094751\tbestTest 0.2475094751\t\ttotal: 5h 2m 40s\tremaining: 12m 36s\n",
      "96:\tlearn 0.2463824132\ttest 0.2474775147\tbestTest 0.2474775147\t\ttotal: 5h 3m 4s\tremaining: 9m 22s\n",
      "97:\tlearn 0.2463500815\ttest 0.2474464292\tbestTest 0.2474464292\t\ttotal: 5h 3m 27s\tremaining: 6m 11s\n",
      "98:\tlearn 0.2463231368\ttest 0.2474215824\tbestTest 0.2474215824\t\ttotal: 5h 3m 52s\tremaining: 3m 4s\n",
      "99:\tlearn 0.246294002\ttest 0.2473932678\tbestTest 0.2473932678\t\ttotal: 5h 4m 16s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2473932678\n",
      "bestIteration = 99\n",
      "\n",
      "Shrink model to first 100 iterations.\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 1.8074820041656494 s \n",
      "\n",
      "1 0.378738295075\n",
      "Borders generated\n",
      "0:\tlearn 0.6347302139\ttest 0.634674958\tbestTest 0.634674958\t\ttotal: 57.2s\tremaining: 1h 34m 21s\n",
      "1:\tlearn 0.5848281981\ttest 0.5847106889\tbestTest 0.5847106889\t\ttotal: 1m 22s\tremaining: 1h 7m 5s\n",
      "2:\tlearn 0.5415780954\ttest 0.5414130342\tbestTest 0.5414130342\t\ttotal: 1m 49s\tremaining: 58m 46s\n",
      "3:\tlearn 0.5045601801\ttest 0.5043692121\tbestTest 0.5043692121\t\ttotal: 2m 16s\tremaining: 54m 38s\n",
      "4:\tlearn 0.4725096632\ttest 0.4722687712\tbestTest 0.4722687712\t\ttotal: 2m 46s\tremaining: 52m 39s\n",
      "5:\tlearn 0.4445771023\ttest 0.4442961067\tbestTest 0.4442961067\t\ttotal: 3m 11s\tremaining: 49m 57s\n",
      "6:\tlearn 0.4203765723\ttest 0.4200788819\tbestTest 0.4200788819\t\ttotal: 3m 36s\tremaining: 47m 54s\n",
      "7:\tlearn 0.3992667299\ttest 0.3989418147\tbestTest 0.3989418147\t\ttotal: 4m\tremaining: 46m 4s\n",
      "8:\tlearn 0.3810245293\ttest 0.3806726048\tbestTest 0.3806726048\t\ttotal: 4m 25s\tremaining: 44m 45s\n",
      "9:\tlearn 0.3653323644\ttest 0.3649912734\tbestTest 0.3649912734\t\ttotal: 4m 49s\tremaining: 43m 24s\n",
      "10:\tlearn 0.3518848654\ttest 0.3515158876\tbestTest 0.3515158876\t\ttotal: 5m 12s\tremaining: 42m 6s\n",
      "11:\tlearn 0.3398747607\ttest 0.3394933515\tbestTest 0.3394933515\t\ttotal: 5m 36s\tremaining: 41m 4s\n",
      "12:\tlearn 0.3295493324\ttest 0.3291550779\tbestTest 0.3291550779\t\ttotal: 5m 59s\tremaining: 40m 6s\n",
      "13:\tlearn 0.3206434974\ttest 0.3202344188\tbestTest 0.3202344188\t\ttotal: 6m 23s\tremaining: 39m 17s\n",
      "14:\tlearn 0.312739885\ttest 0.312321991\tbestTest 0.312321991\t\ttotal: 6m 47s\tremaining: 38m 28s\n",
      "15:\tlearn 0.3057780815\ttest 0.3053614193\tbestTest 0.3053614193\t\ttotal: 7m 11s\tremaining: 37m 47s\n",
      "16:\tlearn 0.2998043769\ttest 0.2993818906\tbestTest 0.2993818906\t\ttotal: 7m 36s\tremaining: 37m 7s\n",
      "17:\tlearn 0.2947287389\ttest 0.2943011669\tbestTest 0.2943011669\t\ttotal: 7m 58s\tremaining: 36m 21s\n",
      "18:\tlearn 0.2899025173\ttest 0.2894706545\tbestTest 0.2894706545\t\ttotal: 8m 22s\tremaining: 35m 43s\n",
      "19:\tlearn 0.2858265818\ttest 0.2853888465\tbestTest 0.2853888465\t\ttotal: 8m 45s\tremaining: 35m 1s\n",
      "20:\tlearn 0.2822444639\ttest 0.2818100476\tbestTest 0.2818100476\t\ttotal: 9m 10s\tremaining: 34m 29s\n",
      "21:\tlearn 0.2789884765\ttest 0.2785573344\tbestTest 0.2785573344\t\ttotal: 9m 33s\tremaining: 33m 54s\n",
      "22:\tlearn 0.276023244\ttest 0.2756002467\tbestTest 0.2756002467\t\ttotal: 9m 57s\tremaining: 33m 21s\n",
      "23:\tlearn 0.2734024557\ttest 0.2729857118\tbestTest 0.2729857118\t\ttotal: 10m 21s\tremaining: 32m 49s\n",
      "24:\tlearn 0.2711500922\ttest 0.2707366799\tbestTest 0.2707366799\t\ttotal: 10m 44s\tremaining: 32m 12s\n",
      "25:\tlearn 0.2689358436\ttest 0.2685242022\tbestTest 0.2685242022\t\ttotal: 11m 7s\tremaining: 31m 40s\n",
      "26:\tlearn 0.2671238667\ttest 0.266717804\tbestTest 0.266717804\t\ttotal: 11m 30s\tremaining: 31m 8s\n",
      "27:\tlearn 0.2654240663\ttest 0.2650264337\tbestTest 0.2650264337\t\ttotal: 11m 54s\tremaining: 30m 36s\n",
      "28:\tlearn 0.2637962991\ttest 0.2633967515\tbestTest 0.2633967515\t\ttotal: 12m 18s\tremaining: 30m 6s\n",
      "29:\tlearn 0.2624653902\ttest 0.2620731012\tbestTest 0.2620731012\t\ttotal: 12m 41s\tremaining: 29m 36s\n",
      "30:\tlearn 0.2611252268\ttest 0.2607363551\tbestTest 0.2607363551\t\ttotal: 13m 4s\tremaining: 29m 6s\n",
      "31:\tlearn 0.2600313987\ttest 0.2596417106\tbestTest 0.2596417106\t\ttotal: 13m 27s\tremaining: 28m 35s\n",
      "32:\tlearn 0.2590671823\ttest 0.258685479\tbestTest 0.258685479\t\ttotal: 33m 56s\tremaining: 1h 8m 55s\n",
      "33:\tlearn 0.2580941198\ttest 0.2577152137\tbestTest 0.2577152137\t\ttotal: 34m 20s\tremaining: 1h 6m 39s\n",
      "34:\tlearn 0.2572920735\ttest 0.2569109949\tbestTest 0.2569109949\t\ttotal: 34m 43s\tremaining: 1h 4m 30s\n",
      "35:\tlearn 0.2566116234\ttest 0.2562325979\tbestTest 0.2562325979\t\ttotal: 35m 7s\tremaining: 1h 2m 27s\n",
      "36:\tlearn 0.2558295538\ttest 0.255458007\tbestTest 0.255458007\t\ttotal: 35m 31s\tremaining: 1h 29s\n",
      "37:\tlearn 0.255175574\ttest 0.2548114947\tbestTest 0.2548114947\t\ttotal: 35m 56s\tremaining: 58m 38s\n",
      "38:\tlearn 0.2545481553\ttest 0.2541895048\tbestTest 0.2541895048\t\ttotal: 36m 20s\tremaining: 56m 50s\n",
      "39:\tlearn 0.254056383\ttest 0.2537032815\tbestTest 0.2537032815\t\ttotal: 36m 44s\tremaining: 55m 6s\n",
      "40:\tlearn 0.2536091173\ttest 0.2532589769\tbestTest 0.2532589769\t\ttotal: 37m 7s\tremaining: 53m 26s\n",
      "41:\tlearn 0.2531015471\ttest 0.252755183\tbestTest 0.252755183\t\ttotal: 37m 31s\tremaining: 51m 49s\n",
      "42:\tlearn 0.2526959342\ttest 0.2523499383\tbestTest 0.2523499383\t\ttotal: 37m 55s\tremaining: 50m 16s\n",
      "43:\tlearn 0.2523211131\ttest 0.2519802203\tbestTest 0.2519802203\t\ttotal: 38m 18s\tremaining: 48m 45s\n",
      "44:\tlearn 0.2519793163\ttest 0.2516420402\tbestTest 0.2516420402\t\ttotal: 38m 44s\tremaining: 47m 21s\n",
      "45:\tlearn 0.2516456531\ttest 0.2513121479\tbestTest 0.2513121479\t\ttotal: 39m 9s\tremaining: 45m 57s\n",
      "46:\tlearn 0.2513565339\ttest 0.2510301463\tbestTest 0.2510301463\t\ttotal: 39m 34s\tremaining: 44m 37s\n",
      "47:\tlearn 0.251033498\ttest 0.250709484\tbestTest 0.250709484\t\ttotal: 40m\tremaining: 43m 20s\n",
      "48:\tlearn 0.2507715084\ttest 0.2504532059\tbestTest 0.2504532059\t\ttotal: 40m 25s\tremaining: 42m 4s\n",
      "49:\tlearn 0.250501254\ttest 0.25018562\tbestTest 0.25018562\t\ttotal: 40m 50s\tremaining: 40m 50s\n",
      "50:\tlearn 0.2502813177\ttest 0.2499755109\tbestTest 0.2499755109\t\ttotal: 41m 15s\tremaining: 39m 37s\n",
      "51:\tlearn 0.2500594875\ttest 0.2497558735\tbestTest 0.2497558735\t\ttotal: 41m 37s\tremaining: 38m 25s\n",
      "52:\tlearn 0.2498352597\ttest 0.2495370037\tbestTest 0.2495370037\t\ttotal: 42m\tremaining: 37m 15s\n",
      "53:\tlearn 0.2496440005\ttest 0.2493498646\tbestTest 0.2493498646\t\ttotal: 42m 24s\tremaining: 36m 7s\n",
      "54:\tlearn 0.2494684516\ttest 0.2491786089\tbestTest 0.2491786089\t\ttotal: 42m 47s\tremaining: 35m\n",
      "55:\tlearn 0.2493017641\ttest 0.249016174\tbestTest 0.249016174\t\ttotal: 43m 12s\tremaining: 33m 57s\n",
      "56:\tlearn 0.2491560082\ttest 0.2488752969\tbestTest 0.2488752969\t\ttotal: 43m 36s\tremaining: 32m 53s\n",
      "57:\tlearn 0.2490198121\ttest 0.2487399917\tbestTest 0.2487399917\t\ttotal: 44m\tremaining: 31m 51s\n",
      "58:\tlearn 0.2488984481\ttest 0.2486288018\tbestTest 0.2486288018\t\ttotal: 44m 23s\tremaining: 30m 50s\n",
      "59:\tlearn 0.2487572883\ttest 0.2484963682\tbestTest 0.2484963682\t\ttotal: 55m 7s\tremaining: 36m 45s\n",
      "60:\tlearn 0.2486478083\ttest 0.2483884318\tbestTest 0.2483884318\t\ttotal: 55m 31s\tremaining: 35m 29s\n",
      "61:\tlearn 0.2485727683\ttest 0.2483217152\tbestTest 0.2483217152\t\ttotal: 55m 55s\tremaining: 34m 16s\n",
      "62:\tlearn 0.248465932\ttest 0.2482154346\tbestTest 0.2482154346\t\ttotal: 56m 20s\tremaining: 33m 5s\n",
      "63:\tlearn 0.2483619244\ttest 0.2481130622\tbestTest 0.2481130622\t\ttotal: 56m 44s\tremaining: 31m 55s\n",
      "64:\tlearn 0.2482526379\ttest 0.2480059026\tbestTest 0.2480059026\t\ttotal: 57m 8s\tremaining: 30m 45s\n",
      "65:\tlearn 0.2481675258\ttest 0.2479256577\tbestTest 0.2479256577\t\ttotal: 57m 32s\tremaining: 29m 38s\n",
      "66:\tlearn 0.2480857658\ttest 0.2478458285\tbestTest 0.2478458285\t\ttotal: 57m 55s\tremaining: 28m 31s\n",
      "67:\tlearn 0.248019458\ttest 0.2477826992\tbestTest 0.2477826992\t\ttotal: 58m 20s\tremaining: 27m 27s\n",
      "68:\tlearn 0.247940697\ttest 0.2477107965\tbestTest 0.2477107965\t\ttotal: 58m 44s\tremaining: 26m 23s\n",
      "69:\tlearn 0.2478580522\ttest 0.247631161\tbestTest 0.247631161\t\ttotal: 59m 10s\tremaining: 25m 21s\n",
      "70:\tlearn 0.2477795776\ttest 0.2475575183\tbestTest 0.2475575183\t\ttotal: 59m 34s\tremaining: 24m 19s\n",
      "71:\tlearn 0.247714953\ttest 0.2474979913\tbestTest 0.2474979913\t\ttotal: 59m 58s\tremaining: 23m 19s\n",
      "72:\tlearn 0.2476728809\ttest 0.2474697529\tbestTest 0.2474697529\t\ttotal: 1h 22s\tremaining: 22m 19s\n",
      "73:\tlearn 0.2476159862\ttest 0.2474132848\tbestTest 0.2474132848\t\ttotal: 1h 46s\tremaining: 21m 21s\n",
      "74:\tlearn 0.2475504347\ttest 0.2473507463\tbestTest 0.2473507463\t\ttotal: 1h 1m 10s\tremaining: 20m 23s\n",
      "75:\tlearn 0.2474912798\ttest 0.2472959168\tbestTest 0.2472959168\t\ttotal: 1h 1m 34s\tremaining: 19m 26s\n",
      "76:\tlearn 0.2474298309\ttest 0.2472382402\tbestTest 0.2472382402\t\ttotal: 1h 1m 57s\tremaining: 18m 30s\n",
      "77:\tlearn 0.2473825171\ttest 0.2471965198\tbestTest 0.2471965198\t\ttotal: 1h 2m 20s\tremaining: 17m 35s\n",
      "78:\tlearn 0.24733101\ttest 0.2471485567\tbestTest 0.2471485567\t\ttotal: 1h 2m 44s\tremaining: 16m 40s\n",
      "79:\tlearn 0.2472900471\ttest 0.2471088415\tbestTest 0.2471088415\t\ttotal: 1h 3m 8s\tremaining: 15m 47s\n",
      "80:\tlearn 0.2472489276\ttest 0.2470713308\tbestTest 0.2470713308\t\ttotal: 1h 3m 32s\tremaining: 14m 54s\n",
      "81:\tlearn 0.2472099294\ttest 0.2470343301\tbestTest 0.2470343301\t\ttotal: 1h 3m 56s\tremaining: 14m 2s\n",
      "82:\tlearn 0.2471691261\ttest 0.2469956775\tbestTest 0.2469956775\t\ttotal: 1h 4m 19s\tremaining: 13m 10s\n",
      "83:\tlearn 0.2471271636\ttest 0.246956827\tbestTest 0.246956827\t\ttotal: 1h 4m 42s\tremaining: 12m 19s\n",
      "84:\tlearn 0.2470938665\ttest 0.2469277964\tbestTest 0.2469277964\t\ttotal: 1h 5m 5s\tremaining: 11m 29s\n",
      "85:\tlearn 0.2470538254\ttest 0.2468938306\tbestTest 0.2468938306\t\ttotal: 1h 5m 28s\tremaining: 10m 39s\n",
      "86:\tlearn 0.2470175939\ttest 0.2468593955\tbestTest 0.2468593955\t\ttotal: 1h 23m 33s\tremaining: 12m 29s\n",
      "87:\tlearn 0.2469835753\ttest 0.2468268791\tbestTest 0.2468268791\t\ttotal: 1h 23m 58s\tremaining: 11m 27s\n",
      "88:\tlearn 0.2469483202\ttest 0.2467938743\tbestTest 0.2467938743\t\ttotal: 1h 24m 22s\tremaining: 10m 25s\n",
      "89:\tlearn 0.2469197804\ttest 0.2467655524\tbestTest 0.2467655524\t\ttotal: 1h 24m 48s\tremaining: 9m 25s\n",
      "90:\tlearn 0.2468799877\ttest 0.2467270288\tbestTest 0.2467270288\t\ttotal: 1h 25m 15s\tremaining: 8m 25s\n",
      "91:\tlearn 0.2468513759\ttest 0.2467000837\tbestTest 0.2467000837\t\ttotal: 1h 25m 39s\tremaining: 7m 26s\n",
      "92:\tlearn 0.2468221702\ttest 0.2466707964\tbestTest 0.2466707964\t\ttotal: 1h 26m 4s\tremaining: 6m 28s\n",
      "93:\tlearn 0.2467843847\ttest 0.2466343791\tbestTest 0.2466343791\t\ttotal: 1h 26m 29s\tremaining: 5m 31s\n",
      "94:\tlearn 0.246756875\ttest 0.2466095017\tbestTest 0.2466095017\t\ttotal: 1h 26m 54s\tremaining: 4m 34s\n",
      "95:\tlearn 0.2467286562\ttest 0.2465826998\tbestTest 0.2465826998\t\ttotal: 1h 27m 18s\tremaining: 3m 38s\n",
      "96:\tlearn 0.2466995865\ttest 0.246556374\tbestTest 0.246556374\t\ttotal: 1h 27m 42s\tremaining: 2m 42s\n",
      "97:\tlearn 0.2466707494\ttest 0.24652863\tbestTest 0.24652863\t\ttotal: 1h 28m 7s\tremaining: 1m 47s\n",
      "98:\tlearn 0.2466457653\ttest 0.2465074297\tbestTest 0.2465074297\t\ttotal: 1h 28m 30s\tremaining: 53.6s\n",
      "99:\tlearn 0.2466269002\ttest 0.2464912336\tbestTest 0.2464912336\t\ttotal: 1h 28m 53s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2464912336\n",
      "bestIteration = 99\n",
      "\n",
      "Shrink model to first 100 iterations.\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 1.7742667198181152 s \n",
      "\n",
      "2 0.37698103457\n",
      "Borders generated\n",
      "0:\tlearn 0.6348396031\ttest 0.6347735166\tbestTest 0.6347735166\t\ttotal: 53.5s\tremaining: 1h 28m 19s\n",
      "1:\tlearn 0.5847253545\ttest 0.584655125\tbestTest 0.584655125\t\ttotal: 1m 17s\tremaining: 1h 3m 1s\n",
      "2:\tlearn 0.5419452637\ttest 0.5418795413\tbestTest 0.5418795413\t\ttotal: 1m 46s\tremaining: 57m 8s\n",
      "3:\tlearn 0.504893207\ttest 0.5048360988\tbestTest 0.5048360988\t\ttotal: 2m 13s\tremaining: 53m 19s\n",
      "4:\tlearn 0.4722123298\ttest 0.4721210468\tbestTest 0.4721210468\t\ttotal: 2m 39s\tremaining: 50m 23s\n",
      "5:\tlearn 0.4443027468\ttest 0.4442118658\tbestTest 0.4442118658\t\ttotal: 3m 3s\tremaining: 48m 1s\n",
      "6:\tlearn 0.4205284277\ttest 0.4204496547\tbestTest 0.4204496547\t\ttotal: 3m 27s\tremaining: 46m 1s\n",
      "7:\tlearn 0.3993954903\ttest 0.3993138209\tbestTest 0.3993138209\t\ttotal: 3m 52s\tremaining: 44m 28s\n",
      "8:\tlearn 0.3812430023\ttest 0.3811682825\tbestTest 0.3811682825\t\ttotal: 4m 16s\tremaining: 43m 17s\n",
      "9:\tlearn 0.3657555237\ttest 0.3656849377\tbestTest 0.3656849377\t\ttotal: 4m 39s\tremaining: 41m 55s\n",
      "10:\tlearn 0.3518860312\ttest 0.3518248578\tbestTest 0.3518248578\t\ttotal: 5m 2s\tremaining: 40m 49s\n",
      "11:\tlearn 0.3398511704\ttest 0.3398203638\tbestTest 0.3398203638\t\ttotal: 5m 25s\tremaining: 39m 44s\n",
      "12:\tlearn 0.3297298904\ttest 0.3297161377\tbestTest 0.3297161377\t\ttotal: 5m 47s\tremaining: 38m 47s\n",
      "13:\tlearn 0.3207311265\ttest 0.3207236285\tbestTest 0.3207236285\t\ttotal: 6m 10s\tremaining: 37m 58s\n",
      "14:\tlearn 0.3127903506\ttest 0.3127939133\tbestTest 0.3127939133\t\ttotal: 6m 34s\tremaining: 37m 12s\n",
      "15:\tlearn 0.3058179476\ttest 0.3058331445\tbestTest 0.3058331445\t\ttotal: 6m 56s\tremaining: 36m 25s\n",
      "16:\tlearn 0.2998068416\ttest 0.2998306323\tbestTest 0.2998306323\t\ttotal: 7m 18s\tremaining: 35m 42s\n",
      "17:\tlearn 0.2943849945\ttest 0.2944103361\tbestTest 0.2944103361\t\ttotal: 7m 41s\tremaining: 35m 1s\n",
      "18:\tlearn 0.2896893683\ttest 0.2897269333\tbestTest 0.2897269333\t\ttotal: 8m 4s\tremaining: 34m 24s\n",
      "19:\tlearn 0.2853716296\ttest 0.2854086486\tbestTest 0.2854086486\t\ttotal: 8m 27s\tremaining: 33m 49s\n",
      "20:\tlearn 0.2814408058\ttest 0.2814802091\tbestTest 0.2814802091\t\ttotal: 8m 50s\tremaining: 33m 13s\n",
      "21:\tlearn 0.2782328484\ttest 0.2782801452\tbestTest 0.2782801452\t\ttotal: 9m 14s\tremaining: 32m 46s\n",
      "22:\tlearn 0.2753100998\ttest 0.2753602651\tbestTest 0.2753602651\t\ttotal: 9m 38s\tremaining: 32m 17s\n",
      "23:\tlearn 0.2726894311\ttest 0.2727444495\tbestTest 0.2727444495\t\ttotal: 10m 2s\tremaining: 31m 46s\n",
      "24:\tlearn 0.2704651716\ttest 0.2705241775\tbestTest 0.2705241775\t\ttotal: 10m 26s\tremaining: 31m 18s\n",
      "25:\tlearn 0.2684298372\ttest 0.2684975254\tbestTest 0.2684975254\t\ttotal: 10m 51s\tremaining: 30m 52s\n",
      "26:\tlearn 0.2666728143\ttest 0.2667459327\tbestTest 0.2667459327\t\ttotal: 11m 15s\tremaining: 30m 25s\n",
      "27:\tlearn 0.264878028\ttest 0.2649525688\tbestTest 0.2649525688\t\ttotal: 11m 39s\tremaining: 29m 58s\n",
      "28:\tlearn 0.2633668215\ttest 0.2634486293\tbestTest 0.2634486293\t\ttotal: 12m 2s\tremaining: 29m 28s\n",
      "29:\tlearn 0.2619169958\ttest 0.2620045987\tbestTest 0.2620045987\t\ttotal: 12m 26s\tremaining: 29m 1s\n",
      "30:\tlearn 0.2607376217\ttest 0.2608301819\tbestTest 0.2608301819\t\ttotal: 12m 49s\tremaining: 28m 33s\n",
      "31:\tlearn 0.2596306751\ttest 0.2597313389\tbestTest 0.2597313389\t\ttotal: 13m 13s\tremaining: 28m 5s\n",
      "32:\tlearn 0.2586118069\ttest 0.2587143606\tbestTest 0.2587143606\t\ttotal: 13m 36s\tremaining: 27m 38s\n",
      "33:\tlearn 0.2576261826\ttest 0.2577306017\tbestTest 0.2577306017\t\ttotal: 14m\tremaining: 27m 11s\n",
      "34:\tlearn 0.2568540987\ttest 0.2569662401\tbestTest 0.2569662401\t\ttotal: 14m 23s\tremaining: 26m 43s\n",
      "35:\tlearn 0.256252527\ttest 0.2563658834\tbestTest 0.2563658834\t\ttotal: 14m 46s\tremaining: 26m 16s\n",
      "36:\tlearn 0.2555563945\ttest 0.2556719957\tbestTest 0.2556719957\t\ttotal: 15m 9s\tremaining: 25m 47s\n",
      "37:\tlearn 0.2549581105\ttest 0.2550784622\tbestTest 0.2550784622\t\ttotal: 15m 32s\tremaining: 25m 20s\n",
      "38:\tlearn 0.2544243026\ttest 0.2545448464\tbestTest 0.2545448464\t\ttotal: 15m 54s\tremaining: 24m 53s\n",
      "39:\tlearn 0.2538578772\ttest 0.2539783596\tbestTest 0.2539783596\t\ttotal: 16m 18s\tremaining: 24m 27s\n",
      "40:\tlearn 0.2533751939\ttest 0.2534953817\tbestTest 0.2534953817\t\ttotal: 16m 42s\tremaining: 24m 2s\n",
      "41:\tlearn 0.2529388682\ttest 0.2530578552\tbestTest 0.2530578552\t\ttotal: 17m 6s\tremaining: 23m 38s\n",
      "42:\tlearn 0.2525854353\ttest 0.2527085152\tbestTest 0.2527085152\t\ttotal: 17m 30s\tremaining: 23m 12s\n",
      "43:\tlearn 0.2521600484\ttest 0.2522839601\tbestTest 0.2522839601\t\ttotal: 17m 54s\tremaining: 22m 47s\n",
      "44:\tlearn 0.251823245\ttest 0.2519507127\tbestTest 0.2519507127\t\ttotal: 18m 19s\tremaining: 22m 23s\n",
      "45:\tlearn 0.2514619713\ttest 0.2515932034\tbestTest 0.2515932034\t\ttotal: 18m 43s\tremaining: 21m 58s\n",
      "46:\tlearn 0.2511881849\ttest 0.2513257377\tbestTest 0.2513257377\t\ttotal: 19m 8s\tremaining: 21m 34s\n",
      "47:\tlearn 0.2509039393\ttest 0.2510424547\tbestTest 0.2510424547\t\ttotal: 19m 32s\tremaining: 21m 10s\n",
      "48:\tlearn 0.2506251183\ttest 0.2507669042\tbestTest 0.2507669042\t\ttotal: 19m 58s\tremaining: 20m 47s\n",
      "49:\tlearn 0.2503749521\ttest 0.250521957\tbestTest 0.250521957\t\ttotal: 20m 24s\tremaining: 20m 24s\n",
      "50:\tlearn 0.2501622821\ttest 0.2503123089\tbestTest 0.2503123089\t\ttotal: 20m 53s\tremaining: 20m 4s\n",
      "51:\tlearn 0.2499915183\ttest 0.2501440079\tbestTest 0.2501440079\t\ttotal: 21m 18s\tremaining: 19m 40s\n",
      "52:\tlearn 0.2498007071\ttest 0.249956088\tbestTest 0.249956088\t\ttotal: 21m 43s\tremaining: 19m 15s\n",
      "53:\tlearn 0.2496660664\ttest 0.2498240843\tbestTest 0.2498240843\t\ttotal: 22m 5s\tremaining: 18m 48s\n",
      "54:\tlearn 0.2495260752\ttest 0.2496819963\tbestTest 0.2496819963\t\ttotal: 22m 29s\tremaining: 18m 23s\n",
      "55:\tlearn 0.2493485197\ttest 0.2495079573\tbestTest 0.2495079573\t\ttotal: 23m 12s\tremaining: 18m 14s\n",
      "56:\tlearn 0.2491696333\ttest 0.2493322567\tbestTest 0.2493322567\t\ttotal: 23m 38s\tremaining: 17m 49s\n",
      "57:\tlearn 0.2490232995\ttest 0.2491851121\tbestTest 0.2491851121\t\ttotal: 24m 4s\tremaining: 17m 26s\n",
      "58:\tlearn 0.2488900675\ttest 0.24905202\tbestTest 0.24905202\t\ttotal: 24m 33s\tremaining: 17m 4s\n",
      "59:\tlearn 0.2487645485\ttest 0.2489303833\tbestTest 0.2489303833\t\ttotal: 25m\tremaining: 16m 40s\n",
      "60:\tlearn 0.248654531\ttest 0.2488244602\tbestTest 0.2488244602\t\ttotal: 25m 29s\tremaining: 16m 18s\n",
      "61:\tlearn 0.248534084\ttest 0.2487048879\tbestTest 0.2487048879\t\ttotal: 25m 55s\tremaining: 15m 53s\n",
      "62:\tlearn 0.2484370332\ttest 0.2486105547\tbestTest 0.2486105547\t\ttotal: 26m 23s\tremaining: 15m 30s\n",
      "63:\tlearn 0.2483260026\ttest 0.2485006517\tbestTest 0.2485006517\t\ttotal: 26m 49s\tremaining: 15m 5s\n",
      "64:\tlearn 0.2482403621\ttest 0.2484162445\tbestTest 0.2484162445\t\ttotal: 27m 16s\tremaining: 14m 41s\n",
      "65:\tlearn 0.2481492093\ttest 0.2483282419\tbestTest 0.2483282419\t\ttotal: 27m 43s\tremaining: 14m 16s\n",
      "66:\tlearn 0.2480622881\ttest 0.2482398965\tbestTest 0.2482398965\t\ttotal: 28m 9s\tremaining: 13m 52s\n",
      "67:\tlearn 0.2479840945\ttest 0.2481639765\tbestTest 0.2481639765\t\ttotal: 28m 36s\tremaining: 13m 27s\n",
      "68:\tlearn 0.2479052021\ttest 0.2480893802\tbestTest 0.2480893802\t\ttotal: 29m 3s\tremaining: 13m 3s\n",
      "69:\tlearn 0.2478274801\ttest 0.2480113439\tbestTest 0.2480113439\t\ttotal: 29m 28s\tremaining: 12m 37s\n",
      "70:\tlearn 0.247750789\ttest 0.2479369\tbestTest 0.2479369\t\ttotal: 29m 57s\tremaining: 12m 13s\n",
      "71:\tlearn 0.2476822624\ttest 0.2478711282\tbestTest 0.2478711282\t\ttotal: 30m 20s\tremaining: 11m 47s\n",
      "72:\tlearn 0.2476095977\ttest 0.2477979862\tbestTest 0.2477979862\t\ttotal: 30m 45s\tremaining: 11m 22s\n",
      "73:\tlearn 0.2475537637\ttest 0.2477455115\tbestTest 0.2477455115\t\ttotal: 31m 9s\tremaining: 10m 56s\n",
      "74:\tlearn 0.2474970157\ttest 0.2476921595\tbestTest 0.2476921595\t\ttotal: 31m 33s\tremaining: 10m 31s\n",
      "75:\tlearn 0.2474424596\ttest 0.2476373235\tbestTest 0.2476373235\t\ttotal: 31m 56s\tremaining: 10m 5s\n",
      "76:\tlearn 0.2474033308\ttest 0.2476019468\tbestTest 0.2476019468\t\ttotal: 32m 20s\tremaining: 9m 39s\n",
      "77:\tlearn 0.2473493534\ttest 0.2475490502\tbestTest 0.2475490502\t\ttotal: 32m 45s\tremaining: 9m 14s\n",
      "78:\tlearn 0.2473039996\ttest 0.2475052365\tbestTest 0.2475052365\t\ttotal: 33m 11s\tremaining: 8m 49s\n",
      "79:\tlearn 0.2472614245\ttest 0.2474639806\tbestTest 0.2474639806\t\ttotal: 33m 38s\tremaining: 8m 24s\n",
      "80:\tlearn 0.2472182447\ttest 0.2474227554\tbestTest 0.2474227554\t\ttotal: 34m 3s\tremaining: 7m 59s\n",
      "81:\tlearn 0.2471826571\ttest 0.2473903824\tbestTest 0.2473903824\t\ttotal: 34m 28s\tremaining: 7m 34s\n",
      "82:\tlearn 0.2471406886\ttest 0.2473502481\tbestTest 0.2473502481\t\ttotal: 34m 53s\tremaining: 7m 8s\n",
      "83:\tlearn 0.2471042439\ttest 0.2473168934\tbestTest 0.2473168934\t\ttotal: 35m 17s\tremaining: 6m 43s\n",
      "84:\tlearn 0.2470658713\ttest 0.2472817556\tbestTest 0.2472817556\t\ttotal: 35m 42s\tremaining: 6m 18s\n",
      "85:\tlearn 0.2470276465\ttest 0.2472447395\tbestTest 0.2472447395\t\ttotal: 36m 11s\tremaining: 5m 53s\n",
      "86:\tlearn 0.2469889625\ttest 0.2472074267\tbestTest 0.2472074267\t\ttotal: 36m 40s\tremaining: 5m 28s\n",
      "87:\tlearn 0.2469454289\ttest 0.247163673\tbestTest 0.247163673\t\ttotal: 37m 8s\tremaining: 5m 3s\n",
      "88:\tlearn 0.246899996\ttest 0.2471188129\tbestTest 0.2471188129\t\ttotal: 37m 35s\tremaining: 4m 38s\n",
      "89:\tlearn 0.2468735822\ttest 0.2470950327\tbestTest 0.2470950327\t\ttotal: 38m 1s\tremaining: 4m 13s\n",
      "90:\tlearn 0.2468388299\ttest 0.247062043\tbestTest 0.247062043\t\ttotal: 38m 26s\tremaining: 3m 48s\n",
      "91:\tlearn 0.2468094156\ttest 0.2470339194\tbestTest 0.2470339194\t\ttotal: 38m 51s\tremaining: 3m 22s\n",
      "92:\tlearn 0.2467753127\ttest 0.2470010146\tbestTest 0.2470010146\t\ttotal: 39m 15s\tremaining: 2m 57s\n",
      "93:\tlearn 0.2467468969\ttest 0.2469727265\tbestTest 0.2469727265\t\ttotal: 39m 40s\tremaining: 2m 31s\n",
      "94:\tlearn 0.2467209912\ttest 0.2469477579\tbestTest 0.2469477579\t\ttotal: 40m 2s\tremaining: 2m 6s\n",
      "95:\tlearn 0.2466912933\ttest 0.2469178179\tbestTest 0.2469178179\t\ttotal: 40m 26s\tremaining: 1m 41s\n",
      "96:\tlearn 0.2466601796\ttest 0.2468862489\tbestTest 0.2468862489\t\ttotal: 40m 49s\tremaining: 1m 15s\n",
      "97:\tlearn 0.2466246865\ttest 0.2468515329\tbestTest 0.2468515329\t\ttotal: 41m 13s\tremaining: 50.5s\n",
      "98:\tlearn 0.246593245\ttest 0.2468219096\tbestTest 0.2468219096\t\ttotal: 41m 38s\tremaining: 25.2s\n",
      "99:\tlearn 0.2465729378\ttest 0.24680374\tbestTest 0.24680374\t\ttotal: 42m 3s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.24680374\n",
      "bestIteration = 99\n",
      "\n",
      "Shrink model to first 100 iterations.\n",
      "add stats features begin ......\n",
      "add stats features end ......\n",
      "time lapsing 2.8609089851379395 s \n",
      "\n",
      "3 0.378693293646\n"
     ]
    }
   ],
   "source": [
    "# cross-validation\n",
    "# thershold = 0.202\n",
    "# convert to form of submit\n",
    "\n",
    "df_cvfolds = []\n",
    "cb = []\n",
    "for fold in range(4):\n",
    "    train_subset = train[train.user_id % 4 != fold]\n",
    "    valid_subset = train[train.user_id % 4 == fold]\n",
    "\n",
    "    X_train = train_subset.drop('reordered', axis=1)\n",
    "    y_train = train_subset.reordered\n",
    "\n",
    "    X_val = valid_subset.drop('reordered', axis=1)\n",
    "    y_val = valid_subset.reordered\n",
    "\n",
    "    val_index = X_val[['user_id', 'product_id', 'order_id']]\n",
    "    \n",
    "    features_to_use = list(X_train.columns)\n",
    "    features_to_use.remove('user_id')\n",
    "    features_to_use.remove('product_id')\n",
    "    features_to_use.remove('order_id')\n",
    "\n",
    "    cb.append(catboost_cv(X_train, y_train, X_val, y_val, features_to_use))\n",
    "    rawpreds = cb[-1].predict_proba(X_val[features_to_use].values)\n",
    "\n",
    "    lim = .202\n",
    "    val_out = val_index.copy()\n",
    "\n",
    "    val_out.loc[:, 'reordered'] = (rawpreds[:,1] > lim).astype(int)\n",
    "    val_out.loc[:, 'product_id'] = val_out.product_id.astype(str)\n",
    "    presubmit = ka_add_groupby_features_n_vs_1(val_out[val_out.reordered == 1], \n",
    "                                               group_columns_list = ['order_id'],\n",
    "                                               target_columns_list = ['product_id'],\n",
    "                                               methods_list = [lambda x: ' '.join(set(x))],\n",
    "                                               keep_only_stats = True)\n",
    "\n",
    "    presubmit = presubmit.set_index('order_id')\n",
    "    presubmit.columns = ['products']\n",
    "\n",
    "    fullfold = pd.DataFrame(index = val_out.order_id.unique())\n",
    "\n",
    "    fullfold.index.name = 'order_id'\n",
    "    fullfold['products'] = ['None'] * len(fullfold)\n",
    "\n",
    "    fullfold.loc[presubmit.index, 'products'] = presubmit.products\n",
    "\n",
    "    print(fold, compare_results(df_train_gt, fullfold))\n",
    "\n",
    "    df_cvfolds.append(fullfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.378174243812\n"
     ]
    }
   ],
   "source": [
    "df_cv = pd.concat(df_cvfolds)\n",
    "print(compare_results(df_train_gt, df_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borders generated\n",
      "0:\tlearn 0.6349675088\ttotal: 1m 16s\tremaining: 2h 6m 1s\n",
      "1:\tlearn 0.584790297\ttotal: 1m 58s\tremaining: 1h 36m 41s\n",
      "2:\tlearn 0.5416821913\ttotal: 2m 43s\tremaining: 1h 28m 15s\n",
      "3:\tlearn 0.5046547768\ttotal: 3m 27s\tremaining: 1h 22m 59s\n",
      "4:\tlearn 0.4722999747\ttotal: 4m 9s\tremaining: 1h 19m 2s\n",
      "5:\tlearn 0.4444803667\ttotal: 4m 48s\tremaining: 1h 15m 17s\n",
      "6:\tlearn 0.420466089\ttotal: 5m 29s\tremaining: 1h 12m 55s\n",
      "7:\tlearn 0.3993038514\ttotal: 6m 11s\tremaining: 1h 11m 13s\n",
      "8:\tlearn 0.3809734197\ttotal: 6m 51s\tremaining: 1h 9m 16s\n",
      "9:\tlearn 0.3652435498\ttotal: 7m 31s\tremaining: 1h 7m 46s\n",
      "10:\tlearn 0.3517755551\ttotal: 8m 13s\tremaining: 1h 6m 31s\n",
      "11:\tlearn 0.3402141839\ttotal: 8m 53s\tremaining: 1h 5m 9s\n",
      "12:\tlearn 0.3300555532\ttotal: 9m 32s\tremaining: 1h 3m 53s\n",
      "13:\tlearn 0.3209402158\ttotal: 10m 12s\tremaining: 1h 2m 44s\n",
      "14:\tlearn 0.3129941312\ttotal: 10m 53s\tremaining: 1h 1m 43s\n",
      "15:\tlearn 0.3059925761\ttotal: 11m 35s\tremaining: 1h 52s\n",
      "16:\tlearn 0.2999866163\ttotal: 12m 18s\tremaining: 1h 5s\n",
      "17:\tlearn 0.2944667246\ttotal: 12m 57s\tremaining: 59m\n",
      "18:\tlearn 0.2898380765\ttotal: 13m 39s\tremaining: 58m 11s\n",
      "19:\tlearn 0.2857892032\ttotal: 14m 17s\tremaining: 57m 11s\n",
      "20:\tlearn 0.2820265978\ttotal: 14m 56s\tremaining: 56m 13s\n",
      "21:\tlearn 0.278944697\ttotal: 15m 36s\tremaining: 55m 19s\n",
      "22:\tlearn 0.2758414517\ttotal: 16m 15s\tremaining: 54m 26s\n",
      "23:\tlearn 0.2732110969\ttotal: 16m 56s\tremaining: 53m 39s\n",
      "24:\tlearn 0.2708414755\ttotal: 17m 35s\tremaining: 52m 46s\n",
      "25:\tlearn 0.2686593896\ttotal: 18m 16s\tremaining: 51m 59s\n",
      "26:\tlearn 0.2667131905\ttotal: 19m 29s\tremaining: 52m 42s\n",
      "27:\tlearn 0.2650821897\ttotal: 20m 15s\tremaining: 52m 5s\n",
      "28:\tlearn 0.263523629\ttotal: 20m 59s\tremaining: 51m 23s\n",
      "29:\tlearn 0.2621962778\ttotal: 21m 42s\tremaining: 50m 38s\n",
      "30:\tlearn 0.2609576222\ttotal: 22m 21s\tremaining: 49m 45s\n",
      "31:\tlearn 0.2598447742\ttotal: 23m 1s\tremaining: 48m 56s\n",
      "32:\tlearn 0.2588515572\ttotal: 23m 42s\tremaining: 48m 7s\n",
      "33:\tlearn 0.2579450183\ttotal: 24m 20s\tremaining: 47m 14s\n",
      "34:\tlearn 0.2570760994\ttotal: 25m 1s\tremaining: 46m 27s\n",
      "35:\tlearn 0.256353765\ttotal: 25m 40s\tremaining: 45m 39s\n",
      "36:\tlearn 0.2556813007\ttotal: 26m 20s\tremaining: 44m 50s\n",
      "37:\tlearn 0.2551058334\ttotal: 26m 59s\tremaining: 44m 2s\n",
      "38:\tlearn 0.2545315069\ttotal: 27m 37s\tremaining: 43m 13s\n",
      "39:\tlearn 0.2540727524\ttotal: 28m 17s\tremaining: 42m 25s\n",
      "40:\tlearn 0.2536682033\ttotal: 28m 57s\tremaining: 41m 40s\n",
      "41:\tlearn 0.2531589155\ttotal: 29m 38s\tremaining: 40m 55s\n",
      "42:\tlearn 0.2527534968\ttotal: 41m 38s\tremaining: 55m 12s\n",
      "43:\tlearn 0.2523474803\ttotal: 42m 30s\tremaining: 54m 5s\n",
      "44:\tlearn 0.25198385\ttotal: 43m 16s\tremaining: 52m 53s\n",
      "45:\tlearn 0.2516247305\ttotal: 43m 56s\tremaining: 51m 35s\n",
      "46:\tlearn 0.2513018371\ttotal: 44m 38s\tremaining: 50m 20s\n",
      "47:\tlearn 0.2510390293\ttotal: 45m 19s\tremaining: 49m 5s\n",
      "48:\tlearn 0.2507983653\ttotal: 45m 58s\tremaining: 47m 50s\n",
      "49:\tlearn 0.2505391888\ttotal: 46m 38s\tremaining: 46m 38s\n",
      "50:\tlearn 0.2502713238\ttotal: 47m 18s\tremaining: 45m 26s\n",
      "51:\tlearn 0.2500306894\ttotal: 48m\tremaining: 44m 19s\n",
      "52:\tlearn 0.2498328599\ttotal: 48m 42s\tremaining: 43m 12s\n",
      "53:\tlearn 0.2496313823\ttotal: 49m 22s\tremaining: 42m 3s\n",
      "54:\tlearn 0.2494434371\ttotal: 50m\tremaining: 40m 54s\n",
      "55:\tlearn 0.2492783479\ttotal: 50m 39s\tremaining: 39m 48s\n",
      "56:\tlearn 0.249145766\ttotal: 51m 20s\tremaining: 38m 43s\n",
      "57:\tlearn 0.2490011124\ttotal: 52m 1s\tremaining: 37m 40s\n",
      "58:\tlearn 0.248861725\ttotal: 52m 42s\tremaining: 36m 37s\n",
      "59:\tlearn 0.2487436942\ttotal: 53m 25s\tremaining: 35m 37s\n",
      "60:\tlearn 0.2486372437\ttotal: 54m 12s\tremaining: 34m 39s\n",
      "61:\tlearn 0.2485424657\ttotal: 54m 55s\tremaining: 33m 40s\n",
      "62:\tlearn 0.2484222686\ttotal: 55m 42s\tremaining: 32m 42s\n",
      "63:\tlearn 0.2483144704\ttotal: 56m 23s\tremaining: 31m 43s\n",
      "64:\tlearn 0.2482234935\ttotal: 57m 7s\tremaining: 30m 45s\n",
      "65:\tlearn 0.2481363887\ttotal: 57m 46s\tremaining: 29m 45s\n",
      "66:\tlearn 0.2480545984\ttotal: 58m 24s\tremaining: 28m 46s\n",
      "67:\tlearn 0.2479769692\ttotal: 59m 3s\tremaining: 27m 47s\n",
      "68:\tlearn 0.247904461\ttotal: 59m 42s\tremaining: 26m 49s\n",
      "69:\tlearn 0.2478407147\ttotal: 1h 26s\tremaining: 25m 54s\n",
      "70:\tlearn 0.2477650439\ttotal: 1h 1m 8s\tremaining: 24m 58s\n",
      "71:\tlearn 0.247691845\ttotal: 1h 1m 47s\tremaining: 24m 1s\n",
      "72:\tlearn 0.2476311632\ttotal: 1h 2m 28s\tremaining: 23m 6s\n",
      "73:\tlearn 0.2475740729\ttotal: 1h 3m 9s\tremaining: 22m 11s\n",
      "74:\tlearn 0.2475227371\ttotal: 1h 3m 47s\tremaining: 21m 15s\n",
      "75:\tlearn 0.2474593896\ttotal: 1h 4m 27s\tremaining: 20m 21s\n",
      "76:\tlearn 0.2474064918\ttotal: 1h 5m 6s\tremaining: 19m 26s\n",
      "77:\tlearn 0.2473556937\ttotal: 1h 5m 47s\tremaining: 18m 33s\n",
      "78:\tlearn 0.2473150709\ttotal: 1h 6m 28s\tremaining: 17m 40s\n",
      "79:\tlearn 0.2472791106\ttotal: 1h 7m 8s\tremaining: 16m 47s\n",
      "80:\tlearn 0.2472369939\ttotal: 1h 7m 47s\tremaining: 15m 54s\n",
      "81:\tlearn 0.2472001534\ttotal: 1h 8m 26s\tremaining: 15m 1s\n",
      "82:\tlearn 0.2471579742\ttotal: 1h 9m 5s\tremaining: 14m 9s\n",
      "83:\tlearn 0.2471118666\ttotal: 1h 9m 45s\tremaining: 13m 17s\n",
      "84:\tlearn 0.2470824291\ttotal: 1h 10m 26s\tremaining: 12m 25s\n",
      "85:\tlearn 0.2470357292\ttotal: 1h 11m 4s\tremaining: 11m 34s\n",
      "86:\tlearn 0.2469946446\ttotal: 1h 11m 47s\tremaining: 10m 43s\n",
      "87:\tlearn 0.2469674875\ttotal: 1h 12m 31s\tremaining: 9m 53s\n",
      "88:\tlearn 0.2469293949\ttotal: 1h 13m 13s\tremaining: 9m 3s\n",
      "89:\tlearn 0.246889003\ttotal: 1h 13m 53s\tremaining: 8m 12s\n",
      "90:\tlearn 0.2468567714\ttotal: 1h 14m 35s\tremaining: 7m 22s\n",
      "91:\tlearn 0.2468284199\ttotal: 1h 15m 14s\tremaining: 6m 32s\n",
      "92:\tlearn 0.2468029764\ttotal: 1h 15m 53s\tremaining: 5m 42s\n",
      "93:\tlearn 0.2467666659\ttotal: 1h 16m 59s\tremaining: 4m 54s\n",
      "94:\tlearn 0.2467363514\ttotal: 1h 17m 48s\tremaining: 4m 5s\n",
      "95:\tlearn 0.246713316\ttotal: 1h 18m 34s\tremaining: 3m 16s\n",
      "96:\tlearn 0.2466882056\ttotal: 1h 19m 17s\tremaining: 2m 27s\n",
      "97:\tlearn 0.2466676465\ttotal: 1h 19m 57s\tremaining: 1m 37s\n",
      "98:\tlearn 0.2466356251\ttotal: 1h 20m 41s\tremaining: 48.9s\n",
      "99:\tlearn 0.2466044965\ttotal: 1h 21m 21s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x15c2cecf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_full = cbc(iterations = 100, \n",
    "              thread_count = 8, \n",
    "              verbose = True, \n",
    "              random_seed = 42,\n",
    "              learning_rate = 0.05,\n",
    "              depth = 8,\n",
    "              fold_permutation_block_size = 64,\n",
    "              calc_feature_importance = True,\n",
    "              leaf_estimation_method = 'Gradient')\n",
    "\n",
    "cb_full.fit(X = train[features_to_use].values, \n",
    "            y = train.reordered.values,\n",
    "            cat_features = [train[features_to_use].columns.get_loc('aisle_id')],\n",
    "            verbose = True,\n",
    "           #plot=True\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000 72053\n"
     ]
    }
   ],
   "source": [
    "testpreds = X_test[['user_id', 'product_id', 'order_id']].copy()\n",
    "testpreds['reordered'] = (cb_full.predict_proba(X_test[features_to_use].values)[:,1] > .202).astype(int)\n",
    "testpreds.product_id = testpreds.product_id.astype(str)\n",
    "\n",
    "g = testpreds[testpreds.reordered == 1].groupby('order_id', sort=False)\n",
    "df_testpreds = g[['product_id']].agg(lambda x: ' '.join(set(x)))\n",
    "\n",
    "# complete (but empty) test df\n",
    "df_test = pd.DataFrame(index = X_test.order_id.unique())\n",
    "df_test.index.name = 'order_id'\n",
    "df_test['products'] = ['None'] * len(df_test)\n",
    "print (len(df_test), len(df_testpreds))\n",
    "\n",
    "# combine empty output df with predictions\n",
    "df_test.loc[df_testpreds.index, 'products'] = df_testpreds.product_id\n",
    "df_test.sort_index(inplace=True)\n",
    "df_test.to_csv('submit_catboost.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
